{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, Embedding, Linear, Dropout, MaxPool1d, Sequential, ReLU\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "\n",
    "class transformer_FFN(Module):\n",
    "    def __init__(self, emb_size, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.dropout = dropout\n",
    "        self.FFN = Sequential(\n",
    "                Linear(self.emb_size, self.emb_size),\n",
    "                ReLU(),\n",
    "                Dropout(self.dropout),\n",
    "                Linear(self.emb_size, self.emb_size),\n",
    "                # Dropout(self.dropout),\n",
    "            )\n",
    "    def forward(self, in_fea):\n",
    "        return self.FFN(in_fea)\n",
    "\n",
    "def ut_mask(seq_len):\n",
    "    \"\"\" Upper Triangular Mask\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(seq_len,seq_len),diagonal=1).to(dtype=torch.bool).to(device)\n",
    "\n",
    "def lt_mask(seq_len):\n",
    "    \"\"\" Upper Triangular Mask\n",
    "    \"\"\"\n",
    "    return torch.tril(torch.ones(seq_len,seq_len),diagonal=-1).to(dtype=torch.bool).to(device)\n",
    "\n",
    "def pos_encode(seq_len):\n",
    "    \"\"\" position Encoding\n",
    "    \"\"\"\n",
    "    return torch.arange(seq_len).unsqueeze(0).to(device)\n",
    "\n",
    "def get_clones(module, N):\n",
    "    \"\"\" Cloning nn modules\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import constant_\n",
    "import torch.nn.init as init\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch.nn.functional as F\n",
    "from enum import IntEnum\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "from torch.nn import Module, Embedding, LSTM, Linear, Dropout, LayerNorm, TransformerEncoder, TransformerEncoderLayer, \\\n",
    "        MultiLabelMarginLoss, MultiLabelSoftMarginLoss, CrossEntropyLoss, BCELoss, MultiheadAttention\n",
    "from torch.nn.functional import one_hot, cross_entropy, multilabel_margin_loss, binary_cross_entropy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2\n",
    "\n",
    "\n",
    "class MoE_CSKT(nn.Module):\n",
    "    def __init__(self, n_question, n_pid, \n",
    "            d_model, n_blocks, dropout, d_ff=256, \n",
    "            loss1=0.5, loss2=0.5, loss3=0.5, start=50, num_layers=2, nheads=4, seq_len=512, r=1, gamma=1, \n",
    "            kq_same=1, final_fc_dim=512, final_fc_dim2=256, num_attn_heads=8, separate_qa=False, l2=1e-5, emb_type=\"qid\", emb_path=\"\", pretrain_dim=768):\n",
    "        super().__init__()\n",
    "        self.n_question = n_question\n",
    "        self.dropout = dropout\n",
    "        self.kq_same = kq_same\n",
    "        self.n_pid = n_pid\n",
    "        self.l2 = l2\n",
    "        self.model_type = self.model_name\n",
    "        self.separate_qa = separate_qa\n",
    "        self.emb_type = emb_type\n",
    "        embed_l = d_model\n",
    "        self.r = r\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if self.n_pid > 0:\n",
    "            if emb_type.find(\"scalar\") != -1: \n",
    "                self.difficult_param = nn.Embedding(self.n_pid+1, 1) \n",
    "            else:\n",
    "                self.difficult_param = nn.Embedding(self.n_pid+1, embed_l)\n",
    "            self.q_embed_diff = nn.Embedding(self.n_question+1, embed_l) \n",
    "            self.qa_embed_diff = nn.Embedding(2 * self.n_question + 1, embed_l) \n",
    "        if emb_type.startswith(\"qid\"):\n",
    "\n",
    "            self.q_embed = nn.Embedding(self.n_question, embed_l)\n",
    "            if self.separate_qa: \n",
    "                    self.qa_embed = nn.Embedding(2*self.n_question+1, embed_l)\n",
    "            else: \n",
    "                self.qa_embed = nn.Embedding(2, embed_l)\n",
    "        self.model = Architecture(n_question=n_question, n_blocks=n_blocks, n_heads=num_attn_heads, dropout=dropout,\n",
    "                                    d_model=d_model, d_feature=d_model / num_attn_heads, d_ff=d_ff,  kq_same=self.kq_same, model_type=self.model_type, seq_len=seq_len, \n",
    "                                    r = r, gamma=gamma)\n",
    "    \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(d_model + embed_l,\n",
    "                      final_fc_dim), nn.ReLU(), nn.Dropout(self.dropout),\n",
    "            nn.Linear(final_fc_dim, final_fc_dim2), nn.ReLU(\n",
    "            ), nn.Dropout(self.dropout),\n",
    "            nn.Linear(final_fc_dim2, 1)\n",
    "        )\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        for p in self.parameters():\n",
    "            if p.size(0) == self.n_pid+1 and self.n_pid > 0:\n",
    "                torch.nn.init.constant_(p, 0.)\n",
    "\n",
    "    def base_emb(self, q_data, target):\n",
    "        q_embed_data = self.q_embed(q_data)  \n",
    "        if self.separate_qa:\n",
    "            qa_data = q_data + self.n_question * target\n",
    "            qa_embed_data = self.qa_embed(qa_data)\n",
    "        else:\n",
    "            qa_embed_data = self.qa_embed(target)+q_embed_data\n",
    "        return q_embed_data, qa_embed_data\n",
    "\n",
    "    def get_attn_pad_mask(self, sm):\n",
    "        batch_size, l = sm.size()\n",
    "        pad_attn_mask = sm.data.eq(0).unsqueeze(1)\n",
    "        pad_attn_mask = pad_attn_mask.expand(batch_size, l, l)\n",
    "        return pad_attn_mask.repeat(self.nhead, 1, 1)\n",
    "\n",
    "    def forward(self, dcur, qtest=False, train=False):\n",
    "\n",
    "        q, c, r = dcur[\"qseqs\"].long(), dcur[\"cseqs\"].long(), dcur[\"rseqs\"].long()\n",
    "        qshft, cshft, rshft = dcur[\"shft_qseqs\"].long(), dcur[\"shft_cseqs\"].long(), dcur[\"shft_rseqs\"].long()\n",
    "        pid_data = torch.cat((q[:,0:1], qshft), dim=1).to(device)\n",
    "        q_data = torch.cat((c[:,0:1], cshft), dim=1).to(device)\n",
    "        target = torch.cat((r[:,0:1], rshft), dim=1).to(device)\n",
    "\n",
    "        emb_type = self.emb_type\n",
    "\n",
    "\n",
    "        if emb_type.startswith(\"qid\"):\n",
    "            q_embed_data, qa_embed_data = self.base_emb(q_data, target)\n",
    "        if self.n_pid > 0 and emb_type.find(\"norasch\") == -1: \n",
    "            if emb_type.find(\"aktrasch\") == -1:\n",
    "                q_embed_diff_data = self.q_embed_diff(q_data)  # \n",
    "                pid_embed_data = self.difficult_param(pid_data)  # \n",
    "                q_embed_data = q_embed_data + pid_embed_data * \\\n",
    "                    q_embed_diff_data \n",
    "\n",
    "            else:\n",
    "                q_embed_diff_data = self.q_embed_diff(q_data)  # \n",
    "                pid_embed_data = self.difficult_param(pid_data)  # \n",
    "                q_embed_data = q_embed_data + pid_embed_data * \\\n",
    "                    q_embed_diff_data  \n",
    "\n",
    "                qa_embed_diff_data = self.qa_embed_diff(\n",
    "                    target)  # \n",
    "                qa_embed_data = qa_embed_data + pid_embed_data * \\\n",
    "                        (qa_embed_diff_data+q_embed_diff_data)  \n",
    "        y2, y3 = 0, 0\n",
    "        if emb_type in [\"qid\", \"qidaktrasch\", \"qid_scalar\", \"qid_norasch\"]:\n",
    "            d_output = self.model(q_embed_data, qa_embed_data)\n",
    "\n",
    "            concat_q = torch.cat([d_output, q_embed_data], dim=-1)\n",
    "            output = self.out(concat_q).squeeze(-1)\n",
    "            m = nn.Sigmoid()\n",
    "            preds = m(output)\n",
    "\n",
    "        if train:\n",
    "            return preds, y2, y3\n",
    "        else:\n",
    "            if qtest:\n",
    "                return preds, concat_q\n",
    "            else:\n",
    "                return preds\n",
    "\n",
    "class Architecture(nn.Module):\n",
    "    def __init__(self, n_question,  n_blocks, d_model, d_feature,\n",
    "                 d_ff, n_heads, dropout, kq_same, model_type, seq_len, r, gamma):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "            n_block : number of stacked blocks in the attention\n",
    "            d_model : dimension of attention input/output\n",
    "            d_feature : dimension of input in each of the multi-head attention part.\n",
    "            n_head : number of heads. n_heads*d_feature = d_model\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.model_type = model_type\n",
    "\n",
    "        if model_type in {'cskt'}:\n",
    "            self.blocks_2 = nn.ModuleList([\n",
    "                TransformerLayer(d_model=d_model, d_feature=d_model // n_heads,\n",
    "                                 d_ff=d_ff, dropout=dropout, n_heads=n_heads, kq_same=kq_same, seq_len = seq_len, r=r, gamma=gamma)\n",
    "                for _ in range(n_blocks)\n",
    "            ])\n",
    "\n",
    "    def forward(self, q_embed_data, qa_embed_data):\n",
    "        seqlen, batch_size = q_embed_data.size(1), q_embed_data.size(0)\n",
    "\n",
    "\n",
    "        qa_pos_embed = qa_embed_data\n",
    "        q_pos_embed = q_embed_data\n",
    "\n",
    "        y = qa_pos_embed\n",
    "        seqlen, batch_size = y.size(1), y.size(0)\n",
    "        x = q_pos_embed\n",
    "        \n",
    "        for block in self.blocks_2:\n",
    "            x = block(mask=0, query=x, key=x, values=y, apply_pos=True) #\n",
    "            \n",
    "        return x\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_feature,\n",
    "                 d_ff, n_heads, dropout,  kq_same, seq_len, r, gamma):\n",
    "        super().__init__()\n",
    "        kq_same = kq_same == 1\n",
    "        self.masked_attn_head = MultiHeadAttention(\n",
    "            d_model, d_feature, n_heads, dropout, kq_same=kq_same, seq_len=seq_len, r=r, gamma=gamma)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, mask, query, key, values, apply_pos=True):\n",
    "\n",
    "        seqlen, batch_size = query.size(1), query.size(0)\n",
    "        nopeek_mask = np.triu(\n",
    "            np.ones((1, 1, seqlen, seqlen)), k=mask).astype('uint8')\n",
    "        src_mask = (torch.from_numpy(nopeek_mask) == 0).to(device)\n",
    "        if mask == 0:  \n",
    "            query2 = self.masked_attn_head(\n",
    "                query, key, values, mask=src_mask, zero_pad=True) \n",
    "        else:\n",
    "            query2 = self.masked_attn_head(\n",
    "                query, key, values, mask=src_mask, zero_pad=False)\n",
    "\n",
    "        query = query + self.dropout1((query2)) \n",
    "        query = self.layer_norm1(query) \n",
    "        if apply_pos:\n",
    "            query2 = self.linear2(self.dropout( \n",
    "                self.activation(self.linear1(query))))\n",
    "            query = query + self.dropout2((query2)) \n",
    "            query = self.layer_norm2(query) \n",
    "        return query\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_feature, n_heads, dropout, kq_same, seq_len, r, gamma, bias=True):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        It has projection layer for getting keys, queries and values. Followed by attention and a connected layer.\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_feature\n",
    "        self.h = n_heads\n",
    "        self.kq_same = kq_same\n",
    "\n",
    "        self.v_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.k_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "        if kq_same is False:\n",
    "            self.q_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.num_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj_bias = bias\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.seq_len = seq_len\n",
    "        self.r = r\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        xavier_uniform_(self.k_linear.weight)\n",
    "        xavier_uniform_(self.v_linear.weight)\n",
    "        if self.kq_same is False:\n",
    "            xavier_uniform_(self.q_linear.weight)\n",
    "\n",
    "        if self.proj_bias:\n",
    "            constant_(self.k_linear.bias, 0.)\n",
    "            constant_(self.v_linear.bias, 0.)\n",
    "            if self.kq_same is False:\n",
    "                constant_(self.q_linear.bias, 0.)\n",
    "            constant_(self.out_proj.bias, 0.)\n",
    "\n",
    "    def forward(self, q, k, v, mask, zero_pad):\n",
    "\n",
    "        bs = q.size(0)\n",
    "\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        if self.kq_same is False:\n",
    "            q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        else:\n",
    "            q = self.k_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        k = k.transpose(1, 2)\n",
    "        q = q.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        scores = attention(q, k, v, self.d_k,\n",
    "                   mask, self.dropout, zero_pad, self.r, self.gamma, alpha=0.5)\n",
    "\n",
    "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "        output = self.out_proj(concat)\n",
    "\n",
    "        return output\n",
    "class AttentionExpert(nn.Module):\n",
    "    def __init__(self, d_model,dropout):\n",
    "        super().__init__()\n",
    "        self.scale = d_model ** 0.5\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B, H, L, D = q.size()\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (q.size(-1) ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        else:\n",
    "            causal_mask = torch.tril(torch.ones(L, L, device=q.device)).bool()\n",
    "            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0).expand(B, H, L, L)\n",
    "            scores = scores.masked_fill(causal_mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = torch.nan_to_num(attn, nan=0.0)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.matmul(attn, v)\n",
    "        output = torch.nan_to_num(output, nan=0.0)\n",
    "        return output\n",
    "\n",
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, d_model, num_experts):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d_model, num_experts)  \n",
    "\n",
    "    def forward(self, q):\n",
    "        B, H, L, D = q.size()\n",
    "        gate_weights = torch.zeros(B, self.fc.out_features, H, L, 1, device=q.device)\n",
    "        \n",
    "        q_causal = torch.zeros_like(q)\n",
    "        for t in range(L):\n",
    "            if t == 0:\n",
    "                q_causal[:, :, t, :] = q[:, :, t, :].detach()\n",
    "            else:\n",
    "                past_mean = q[:, :, :t+1, :].mean(dim=2).detach()\n",
    "                q_causal[:, :, t, :] = past_mean\n",
    "        for t in range(L):\n",
    "            q_t = q_causal[:, :, t, :]  \n",
    "            q_reshape = q_t.reshape(B * H, D)  \n",
    "            gate_logits = self.fc(q_reshape) \n",
    "            gate_t = F.softmax(gate_logits, dim=-1) \n",
    "            gate_t = gate_t.view(B, H, -1).permute(0, 2, 1).unsqueeze(-1)\n",
    "            gate_weights[:, :, :, t, :] = gate_t \n",
    "\n",
    "        return gate_weights\n",
    "\n",
    "class SemanticRetNet(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.scale = d_model ** 0.5\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B, H, L, D = q.size()\n",
    "        output = torch.zeros_like(v)\n",
    "        y = torch.zeros(B, H, D, device=q.device)\n",
    "\n",
    "        if mask is not None:\n",
    "            diag_mask = mask[:, :, torch.arange(L), torch.arange(L)]  # (B, H, L)\n",
    "\n",
    "        for t in range(L):\n",
    "            q_t = q[:, :, t, :]\n",
    "            k_t = k[:, :, t, :]\n",
    "            v_t = self.dropout(v[:, :, t, :])\n",
    "\n",
    "            sim = (q_t * k_t).sum(dim=-1, keepdim=True) / self.scale\n",
    "            decay_t = self.sigmoid(sim)\n",
    "\n",
    "            if mask is not None:\n",
    "                mask_t = diag_mask[:, :, t].unsqueeze(-1)  # (B, H, 1)\n",
    "                v_t = v_t * mask_t  # padding 位置 v_t = 0\n",
    "\n",
    "            y = decay_t * y + v_t\n",
    "            output[:, :, t, :] = y\n",
    "\n",
    "        return output       \n",
    "class DecayRetNet(nn.Module):\n",
    "    def __init__(self, d_model,decay=0.9):\n",
    "        super().__init__()\n",
    "        self.decay = decay\n",
    "    def forward(self, q, k, v, mask=None, decay=0.9):\n",
    "        batch, head, seq_len, d_k = q.size()\n",
    "        output = torch.zeros_like(v)\n",
    "        y = torch.zeros(batch, head, d_k).to(q.device)\n",
    "        decay = max(min(decay, 1.0), 0.0)\n",
    "        \n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = v[:, :, t, :]\n",
    "            y = decay * y + x_t\n",
    "            output[:, :, t, :] = y\n",
    "    \n",
    "    \n",
    "        return output\n",
    "class MoA(nn.Module):\n",
    "    def __init__(self, d_model, num_qk_experts=2, use_retnet=True, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleList([\n",
    "            AttentionExpert(d_model, dropout=dropout) for _ in range(num_qk_experts)\n",
    "        ])\n",
    "        if use_retnet:\n",
    "            self.experts.append(SemanticRetNet(d_model))\n",
    "            self.experts.append(DecayRetNet(d_model))\n",
    "        self.gating = GatingNetwork(d_model, len(self.experts)) \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        expert_outputs = [expert(q, k, v, mask) for expert in self.experts]\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=1)\n",
    "        gate_weights = self.gating(q)\n",
    "        return (gate_weights * expert_outputs).sum(dim=1)\n",
    "\n",
    "def attention(q, k, v, d_k, mask, dropout, zero_pad, r, gamma, correct=None, alpha=0.5):\n",
    "\n",
    "    d_model = q.shape[-1]\n",
    "    moa_layer = MoA(d_model=d_model, num_qk_experts=2, use_retnet=True,dropout=dropout).to(q.device)\n",
    "    output = moa_layer(q, k, v, mask=mask)\n",
    "    return output\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7460283,
     "sourceId": 11871229,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
